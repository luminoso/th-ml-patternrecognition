{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'scalogram'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-a5ed28d12f00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmultiprocessing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mscalogram\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'scalogram'"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import pickle\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from blist import blist\n",
    "from scipy import stats\n",
    "from sklearn.feature_extraction import image\n",
    "\n",
    "from time_utils import Timebar\n",
    "import multiprocessing\n",
    "\n",
    "import scalogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns names aka feature names\n",
    "def cols_names():\n",
    "    cols = ['entity']\n",
    "\n",
    "    feature_descriptions = ['packet_count',\n",
    "                            'ip_external',\n",
    "                            'ip_internal',\n",
    "                            'port_high',\n",
    "                            'port_low',\n",
    "                            'tcp_syn',\n",
    "                            'tcp_fin',\n",
    "                            'tcp_rst',\n",
    "                            'volume_down',\n",
    "                            'volume_up',\n",
    "                            'volume_internal',\n",
    "                            'less_64kb']\n",
    "\n",
    "    feature_descriptions_combinations = ['mean', 'skew', 'kurtosis', 'var']\n",
    "\n",
    "    for d in feature_descriptions:\n",
    "        for f in feature_descriptions_combinations:\n",
    "            cols.append(f\"{d}:{f}\")\n",
    "\n",
    "    cols.append('count_nonzero')\n",
    "    cols.append('relative_day_position')\n",
    "    #cols.append('is_weekday')\n",
    "    cols.append('silent_occr')\n",
    "    cols.append('silent_mean')\n",
    "    cols.append('silent_var')\n",
    "\n",
    "    for i in range(0,10):\n",
    "        cols.append(f\"wavelet_{i}\")\n",
    "\n",
    "    return cols\n",
    "\n",
    "#def compute_features(raw_files: list = [], split_size: int = 24, step_size: float = 1 / 3, skip_weekends: bool = True):\n",
    "def compute_features(params):\n",
    "\n",
    "    np_file, split_size = params\n",
    "\n",
    "    features = blist([])\n",
    "\n",
    "    nr_splits = 86400 / split_size\n",
    "    step_size = split_size * 1/3\n",
    "\n",
    "    print(f\"Nr_splits: {nr_splits}, step_size: {time.strftime('%H:%M:%S', time.gmtime(step_size))} ({int(step_size)})\")\n",
    "\n",
    "    print(f\"{np_file}:{split_size} Loading {np_file}...\")\n",
    "    days = pickle.load(open(np_file, 'rb'))  # type: np.ndarray\n",
    "    print(f\"{np_file}:{split_size} done loading.\")\n",
    "\n",
    "    nr_days = 0\n",
    "    nr_blocks = 0\n",
    "    nr_skip_days = 0\n",
    "\n",
    "    for timebar_lst in days:\n",
    "\n",
    "        split = image.extract_patches_2d(timebar_lst, (int(len(timebar_lst) / nr_splits), timebar_lst.shape[1]))\n",
    "\n",
    "        # calculate features for each day slice\n",
    "        relative_day_position = 0\n",
    "\n",
    "        for day_block in itertools.islice(split, 0, len(split), int(step_size)):\n",
    "            # try to slide 1 minute in the block of nr_splits\n",
    "            # try:\n",
    "            #    for _ in range(0,60):\n",
    "            #        day_block = next(it)\n",
    "            # except StopIteration:\n",
    "            #    pass\n",
    "\n",
    "            nr_blocks = nr_blocks + 1\n",
    "\n",
    "            name = np_file\n",
    "            feature1 = np.mean(day_block, axis=0)\n",
    "            ## feature2 = np.average(day_block, axis=0) # average is mean with more options\n",
    "            feature3 = stats.skew(day_block, axis=0)\n",
    "            feature4 = stats.kurtosis(day_block, axis=0)\n",
    "            feature5 = np.var(day_block, axis=0)\n",
    "            feature6 = np.count_nonzero(day_block)\n",
    "            feature7 = relative_day_position\n",
    "            #is_weekday = 1\n",
    "            relative_day_position = relative_day_position + 1\n",
    "\n",
    "            silent_periods = []\n",
    "            silent_counter = 0\n",
    "\n",
    "            pckts_arr = day_block[:, 0]\n",
    "\n",
    "            for n in pckts_arr:\n",
    "                if n != 0:\n",
    "                    if silent_counter != 0:\n",
    "                        silent_periods.append(silent_counter)\n",
    "                    silent_counter = 0\n",
    "                else:\n",
    "                    silent_counter = silent_counter + 1\n",
    "\n",
    "            if silent_counter != 0:\n",
    "                if silent_counter == len(silent_periods):\n",
    "                    silent_periods.append(0)\n",
    "                else:\n",
    "                    silent_periods.append(silent_counter)\n",
    "\n",
    "            silent_occr = len(silent_periods)\n",
    "            silent_mean = np.mean(silent_periods) if silent_periods else 0\n",
    "            silent_var = np.var(silent_periods) if silent_periods else 0\n",
    "\n",
    "            scales = np.arange(1, int(np.floor(np.sqrt(pckts_arr.shape[0] / 2))))\n",
    "\n",
    "            if np.sum(pckts_arr) == 0:\n",
    "                S = np.zeros(scales.shape[0])\n",
    "            else:\n",
    "                S, _ = scalogram.scalogramCWT(pckts_arr, scales)\n",
    "\n",
    "            indices = np.linspace(0, S.shape[0] - 1, 10).astype(int)\n",
    "            feature11 = np.take(S, indices)\n",
    "\n",
    "            # append feature line\n",
    "            features.append(np.hstack(\n",
    "                (name, feature1, feature3, feature4, feature5, feature6, feature7,\n",
    "                 #is_weekday,\n",
    "                 silent_occr,\n",
    "                 silent_mean,\n",
    "                 silent_var,\n",
    "                 feature11)))\n",
    "\n",
    "        nr_days = nr_days + 1\n",
    "\n",
    "        print(f\"- {np_file}:{split_size} {nr_days} days splitted in {nr_blocks} blocks, skipped {nr_skip_days} days.\")\n",
    "\n",
    "    print(f\"{np_file}:{split_size} Creating df...\")\n",
    "\n",
    "    df = pd.DataFrame(list(features), columns=cols_names())\n",
    "\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    filename = f\"df_{split_size}m_0.3step_{np_file}-v3.raw\"\n",
    "\n",
    "    print(f\"{np_file}:{split_size} Dumping DataFrame {df.shape} table to {filename}...\")\n",
    "\n",
    "    pickle.dump(df, open(filename, \"wb\"))\n",
    "\n",
    "    print(f\"{np_file}:{split_size} Done.\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "    #return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_files = [\n",
    "    \"np_freebsd.raw\",\n",
    "    \"np_nuc.raw\",\n",
    "    \"np_tohiba.raw\",\n",
    "    \"np_ubuntu.raw\",\n",
    "    \"np_windows10x86.raw\",\n",
    "    \"np_x58pc.raw\"\n",
    "]\n",
    "\n",
    "workers = []\n",
    "\n",
    "# 24 horas 1440  1 blocos 86400\n",
    "# 6 horas   360  4 blocos 21600\n",
    "# 2 horas   120 12 blocos  7200\n",
    "# 1 hora     60 24 blocos  3600\n",
    "# 15 min     15 96 blocos   900\n",
    "# nr_splits = (split_size / 60) / 24\n",
    "\n",
    "\n",
    "for split_size in [86400, 21600, 7200, 3600, 900]:\n",
    "\n",
    "    for np_file in raw_files:\n",
    "\n",
    "        w = multiprocessing.Process(\n",
    "            name=f\"{np_file}:{split_size}\",\n",
    "            target=compute_features,\n",
    "            args=((np_file,split_size),)\n",
    "        )\n",
    "\n",
    "        workers.append(w)\n",
    "\n",
    "for i in workers:\n",
    "    i.start()\n",
    "\n",
    "for i in workers:\n",
    "    i.join()\n",
    "\n",
    "    #features = compute_features(raw_files, split_size, step_size=1 / 3, skip_weekends=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
