{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load serialized sniffed timebar-data and convert it to Panda's Dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sniffed machine:\n",
    "1. Load the sniffed data that is now arranged in `Timebar`s\n",
    "2. Unroll `Timebar` counter values to a list (up to 86400 lines - aggregation of every second)\n",
    "3. Split the unrolled list in, e.g., 24 blocks (features of every hour)\n",
    "4. Convert do Panda's DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "from time_utils import Timebar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(raw_files: list = [], nr_splits: int = 24, skip_weekends: bool = True):\n",
    "    \"\"\"\n",
    "    Computes features for the sniffed aggregated data\n",
    "    :param raw_files: list of files containing serialized pickle Timebar data \n",
    "    :param nr_splits: Split each day in how many blocks\n",
    "    :param skip_weekends: Compute features only from monday to friday\n",
    "    :return: list with all features from all raw_files\n",
    "    \"\"\"\n",
    "\n",
    "    raw_data = []  # list containing all features\n",
    "\n",
    "    for file in raw_files:\n",
    "\n",
    "        print(f\"Loading {file}...\", end=\" \")\n",
    "        days = pickle.load(open(file, 'rb'))  # type: list[Timebar]\n",
    "        print(\"done.\",end=\"\\t\")\n",
    "        \n",
    "        nr_days = 0\n",
    "        nr_blocks = 0\n",
    "        nr_skip_days = 0\n",
    "\n",
    "        day = None  # type: Timebar\n",
    "\n",
    "        for day in days:\n",
    "\n",
    "            # check if day is empty\n",
    "            if day.is_empty():\n",
    "                nr_skip_days = nr_skip_days + 1\n",
    "                continue\n",
    "\n",
    "            if day.is_weekend() and skip_weekends:\n",
    "                nr_skip_days = nr_skip_days + 1\n",
    "                continue\n",
    "\n",
    "            timebar_lst = day.unroll_to_lst()\n",
    "            np_tb = np.array(timebar_lst)\n",
    "\n",
    "            # 2.split the whole day in how many blocks? 24 for every hour. 96 for every 15 minutes\n",
    "            split = np.array_split(np_tb, nr_splits)\n",
    "\n",
    "            # calculate features for each day slice\n",
    "            relative_day_position = 0\n",
    "            for day_block in split:\n",
    "                nr_blocks = nr_blocks + 1\n",
    "\n",
    "                name = file\n",
    "                feature1 = np.mean(day_block, axis=0)\n",
    "                #feature2 = np.average(day_block, axis=0) # average is mean with more options\n",
    "                feature3 = stats.skew(day_block, axis=0)\n",
    "                feature4 = stats.kurtosis(day_block, axis=0)\n",
    "                feature5 = np.var(day_block, axis=0)\n",
    "                feature6 = np.count_nonzero(day_block)\n",
    "                feature7 = relative_day_position\n",
    "                is_weekday = 0 if day.is_weekend() else 1\n",
    "                \n",
    "                relative_day_position = relative_day_position + 1\n",
    "\n",
    "                # append feature line\n",
    "                raw_data.append(\n",
    "                    np.hstack((name, feature1, feature3, feature4, feature5, feature6, feature7, is_weekday)))\n",
    "\n",
    "            nr_days = nr_days + 1\n",
    "\n",
    "        print(f\"{nr_days} days splitted in {nr_blocks} blocks and skipped {nr_skip_days} days.\")\n",
    "\n",
    "    return raw_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split size: 1\n",
      "Loading counters_freebsd.raw... done.\t32 days splitted in 32 blocks and skipped 335 days.\n",
      "Loading counters_windows10x86.raw... done.\t27 days splitted in 27 blocks and skipped 340 days.\n",
      "Dumping DataFrame (59, 52) table to df_1.raw... Done.\n",
      "\n",
      "Split size: 4\n",
      "Loading counters_freebsd.raw... done.\t32 days splitted in 128 blocks and skipped 335 days.\n",
      "Loading counters_windows10x86.raw... done.\t27 days splitted in 108 blocks and skipped 340 days.\n",
      "Dumping DataFrame (236, 52) table to df_4.raw... Done.\n",
      "\n",
      "Split size: 12\n",
      "Loading counters_freebsd.raw... done.\t32 days splitted in 384 blocks and skipped 335 days.\n",
      "Loading counters_windows10x86.raw... done.\t27 days splitted in 324 blocks and skipped 340 days.\n",
      "Dumping DataFrame (708, 52) table to df_12.raw... Done.\n",
      "\n",
      "Split size: 24\n",
      "Loading counters_freebsd.raw... done.\t32 days splitted in 768 blocks and skipped 335 days.\n",
      "Loading counters_windows10x86.raw... done.\t27 days splitted in 648 blocks and skipped 340 days.\n",
      "Dumping DataFrame (1416, 52) table to df_24.raw... Done.\n",
      "\n",
      "Split size: 96\n",
      "Loading counters_freebsd.raw... done.\t32 days splitted in 3072 blocks and skipped 335 days.\n",
      "Loading counters_windows10x86.raw... done.\t27 days splitted in 2592 blocks and skipped 340 days.\n",
      "Dumping DataFrame (5664, 52) table to df_96.raw... Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "raw_files = [\n",
    "    \"counters_freebsd.raw\",\n",
    "    \"counters_toshiba.raw\",\n",
    "    \"counters_windows10x86.raw\",\n",
    "    \"counters_nuc.raw\",\n",
    "    \"counters_ubuntu.raw\",\n",
    "    \"counters_x58pc.raw\"\n",
    "]\n",
    "\n",
    "raw_files = [\n",
    "    \"counters_freebsd.raw\",\n",
    "    \"counters_windows10x86.raw\",\n",
    "]\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# columns names aka feature names\n",
    "\n",
    "feature_labels = ['entity']\n",
    "\n",
    "feature_descriptions = ['packet_count',\n",
    "     'ip_external',\n",
    "     'ip_internal',\n",
    "     'port_high',\n",
    "     'port_low',\n",
    "     'tcp_syn',\n",
    "     'tcp_fin',\n",
    "     'tcp_rst',\n",
    "     'volume_down',\n",
    "     'volume_up',\n",
    "     'volume_internal',\n",
    "     'less_64kb']\n",
    "\n",
    "feature_descriptions_combinations = ['mean', 'skew', 'kurtosis', 'var']\n",
    "\n",
    "for d in feature_descriptions:\n",
    "    for f in feature_descriptions_combinations:\n",
    "        feature_labels.append(f\"{d}:{f}\")\n",
    "\n",
    "feature_labels.append('count_nonzero')\n",
    "feature_labels.append('relative_day_position')\n",
    "feature_labels.append('is_weekday')\n",
    "\n",
    "\n",
    "for nr_splits in [1, 4, 12, 24, 96]:\n",
    "    print(f\"Split size: {nr_splits}\")\n",
    "\n",
    "    raw_data = compute_features(raw_files, nr_splits, skip_weekends=True)\n",
    "\n",
    "    # convert to panda dataframe and guess types\n",
    "    df = pd.DataFrame(raw_data, columns=feature_labels)\n",
    "    df = df.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    filename = f\"df_{nr_splits}.raw\"\n",
    "\n",
    "    print(f\"Dumping DataFrame {df.shape} table to {filename}...\", end=\" \")\n",
    "\n",
    "    pickle.dump(df, open(filename, \"wb\"))\n",
    "\n",
    "    print(\"Done.\", end=\"\\n\\n\")\n",
    "\n",
    "#df_backup = df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_labels = ['entity']\n",
    "\n",
    "feature_descriptions = ['packet_count',\n",
    "     'ip_external',\n",
    "     'ip_internal',\n",
    "     'port_high',\n",
    "     'port_low',\n",
    "     'tcp_syn',\n",
    "     'tcp_fin',\n",
    "     'tcp_rst',\n",
    "     'volume_down',\n",
    "     'volume_up',\n",
    "     'volume_internal',\n",
    "     'less_64kb']\n",
    "\n",
    "feature_descriptions_combinations = ['mean', 'skew', 'kurtosis', 'var']\n",
    "\n",
    "for d in feature_descriptions:\n",
    "    for f in feature_descriptions_combinations:\n",
    "        feature_labels.append(f\"{d}:{f}\")\n",
    "\n",
    "feature_labels.append('count_nonzero')\n",
    "feature_labels.append('relative_day_position')\n",
    "feature_labels.append('is_weekday')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['entity',\n",
       " 'packet_count:mean',\n",
       " 'packet_count:skew',\n",
       " 'packet_count:kurtosis',\n",
       " 'packet_count:var',\n",
       " 'ip_external:mean',\n",
       " 'ip_external:skew',\n",
       " 'ip_external:kurtosis',\n",
       " 'ip_external:var',\n",
       " 'ip_internal:mean',\n",
       " 'ip_internal:skew',\n",
       " 'ip_internal:kurtosis',\n",
       " 'ip_internal:var',\n",
       " 'port_high:mean',\n",
       " 'port_high:skew',\n",
       " 'port_high:kurtosis',\n",
       " 'port_high:var',\n",
       " 'port_low:mean',\n",
       " 'port_low:skew',\n",
       " 'port_low:kurtosis',\n",
       " 'port_low:var',\n",
       " 'tcp_syn:mean',\n",
       " 'tcp_syn:skew',\n",
       " 'tcp_syn:kurtosis',\n",
       " 'tcp_syn:var',\n",
       " 'tcp_fin:mean',\n",
       " 'tcp_fin:skew',\n",
       " 'tcp_fin:kurtosis',\n",
       " 'tcp_fin:var',\n",
       " 'tcp_rst:mean',\n",
       " 'tcp_rst:skew',\n",
       " 'tcp_rst:kurtosis',\n",
       " 'tcp_rst:var',\n",
       " 'volume_down:mean',\n",
       " 'volume_down:skew',\n",
       " 'volume_down:kurtosis',\n",
       " 'volume_down:var',\n",
       " 'volume_up:mean',\n",
       " 'volume_up:skew',\n",
       " 'volume_up:kurtosis',\n",
       " 'volume_up:var',\n",
       " 'volume_internal:mean',\n",
       " 'volume_internal:skew',\n",
       " 'volume_internal:kurtosis',\n",
       " 'volume_internal:var',\n",
       " 'less_64kb:mean',\n",
       " 'less_64kb:skew',\n",
       " 'less_64kb:kurtosis',\n",
       " 'less_64kb:var',\n",
       " 'count_nonzero',\n",
       " 'relative_day_position',\n",
       " 'is_weekday']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
